<!DOCTYPE html>
<html>
    <head>
        <title> The Feral Jalapeno Blogs</title>
    </head>
    <body>
        
      
       

       <hr>
       <h2><center>Why was Netscape Revolutionary?</center></h2>
       Besides the simplicity of the browser’s point and click system, it offered something more important – security.
       <br>
        Netscape Navigator, version 1.0, worked with engineer, Kipp Hickman, which provided the browser with unpresented security. The browser pioneered SSL protocol which established and encrypted a link between browser and server. This allowed for consumer protection. The underlying technology of SSL powers todays security standard – TLS (transport layer security). This was a solid method of authenticating two end points and having a secure connection.
        <br>
        Netscape Navigator, version 2.0, released soon after. This version functioned faster and incorporated Java applets which then allowed for plug-ins (a software component that adds another feature to an existing computer program). The browser had millions of users and had 80% market share.
        <br>
        <hr>
        <h2><center>Web Robots- Legitimate and Malicious</center></h2>
        <br>
        Robots… an interesting word. It’ll either make you think about Terminator or if you’re, from South Africa, traffic lights. I still don’t know why we call them that.
        <br><br>
        But I’m talking about Web Robots. The ones that don’t have red eyes to incinerate you or force you to push the brakes on your car. Web robots are essentially just a set of programs or a simple software application that transverse any web application and runs through a variety of automated tasks. This is mainly repetitive tasks that the robots can do much faster than if a human were to do it manually. They are nothing but scripts which gather information from web servers and digest and sort out this information at a rate humans could never aspire to.
        <br><br>
        We develop web bots using different open source programming languages which allow web robot developers to code scripts for the robots to perform specific tasks. There are two main types of web robots, legitimate and malicious.
        <br><br>
        Legitimate web robots: these kinds of robots are meant to save time and effort for users. Like it was said before, they run tasks that would waste a lot of time if done manually. Unlike some people, these robots actually serve a purpose and save time (ouch ). 
        <br><br>
        An example of a legitimate bot would be a Media Web Bot, like the ones that upload the weather conditions and forecasts. Sorry to disappoint you but there isn’t some guy standing outside sending you updates on the weather conditions for the week, it’s a web bot. So now we have a legitimate web bot to thank for reminding us to take an umbrella. I supposed we could also thank the human that created the bot, but where’s the fun in that?
        <br><br>
        Malicious Web robots: Mostly used by hackers … literally who else would use something called a malicious web robot? The robots are used with the purpose of malicious acts and security breaches. so obviously they’re used by hackers, in case the word malicious isn’t convincing enough.
        <br><br>
        Here’s an example that makes all of us want to smash our screens in, spam web robots. These bots are used for spreading advertisements with pop-ups. So essentially their purpose is to annoy the hell out of us when we’re trying to illegally download music from our favourite artist, that we love SO much but not enough to actually pay for their music… ahem anyway. These bots are the ones responsible for collecting email addresses which lead to peoples’ inboxes being filled with advertisements as well as their long lost, Nigerian prince cousins finally contacting them about the amount of gold they wish to send them.
        <br><br>
        <hr>
       




        

    </body>
</html>